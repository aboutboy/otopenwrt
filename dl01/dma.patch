diff -Nru linux-3.10.14/drivers/dma/iovlock.c linux-3.10.14.new/drivers/dma/iovlock.c
--- linux-3.10.14/drivers/dma/iovlock.c	2013-10-02 00:18:05.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/iovlock.c	2015-12-31 11:55:16.000000000 +0800
@@ -121,6 +121,73 @@
 	return NULL;
 }
 
+#if defined (CONFIG_SPLICE_NET_SUPPORT)
+struct dma_pinned_list *dma_pin_kernel_iovec_pages(struct iovec *iov, size_t len)
+{
+	struct dma_pinned_list *local_list;
+	struct page **pages;
+	int i, j;
+	int nr_iovecs = 0;
+	int iovec_len_used = 0;
+	int iovec_pages_used = 0;
+
+	/* determine how many iovecs/pages there are, up front */
+	do {
+		iovec_len_used += iov[nr_iovecs].iov_len;
+		iovec_pages_used += num_pages_spanned(&iov[nr_iovecs]);
+		nr_iovecs++;
+	} while (iovec_len_used < len);
+
+	/* single kmalloc for pinned list, page_list[], and the page arrays */
+	local_list = kmalloc(sizeof(*local_list)
+		+ (nr_iovecs * sizeof (struct dma_page_list))
+		+ (iovec_pages_used * sizeof (struct page*)), GFP_KERNEL);
+	if (!local_list)
+		goto out;
+
+	/* list of pages starts right after the page list array */
+	pages = (struct page **) &local_list->page_list[nr_iovecs];
+
+	local_list->nr_iovecs = 0;
+
+	for (i = 0; i < nr_iovecs; i++) {
+		struct dma_page_list *page_list = &local_list->page_list[i];
+		int offset;
+
+		len -= iov[i].iov_len;
+
+		if (!access_ok(VERIFY_WRITE, iov[i].iov_base, iov[i].iov_len))
+			goto unpin;
+
+		page_list->nr_pages = num_pages_spanned(&iov[i]);
+		page_list->base_address = iov[i].iov_base;
+		page_list->pages = pages;
+		pages += page_list->nr_pages;
+
+		for (offset=0, j=0; j < page_list->nr_pages; j++, offset+=PAGE_SIZE)
+			page_list->pages[j] = virt_to_page(page_list->base_address + offset);
+		local_list->nr_iovecs = i + 1;
+	}
+
+	return local_list;
+
+unpin:
+	kfree(local_list);
+out:
+	return NULL;
+}
+
+void dma_unpin_kernel_iovec_pages(struct dma_pinned_list *pinned_list)
+{
+	int i, j;
+
+	if (!pinned_list)
+		return;
+
+	kfree(pinned_list);
+}
+#endif
+
 void dma_unpin_iovec_pages(struct dma_pinned_list *pinned_list)
 {
 	int i, j;
diff -Nru linux-3.10.14/drivers/dma/Kconfig linux-3.10.14.new/drivers/dma/Kconfig
--- linux-3.10.14/drivers/dma/Kconfig	2013-10-02 00:18:05.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/Kconfig	2015-12-31 11:55:16.000000000 +0800
@@ -133,6 +133,22 @@
 	---help---
 	  Enable support for the Marvell XOR engine.
 
+config RT_DMA
+        bool "Ralink DMA engine support"
+        select DMA_ENGINE
+        ---help---
+          Enable support for the Ralink DMA engine.
+choice
+        prompt "DMA TYPE"
+
+config RT_DMA_HSDMA
+        bool "High speed DMA"
+
+config RT_DMA_GDMA
+        bool "Generic DMA"
+
+endchoice
+
 config MX3_IPU
 	bool "MX3x Image Processing Unit support"
 	depends on ARCH_MXC
diff -Nru linux-3.10.14/drivers/dma/Makefile linux-3.10.14.new/drivers/dma/Makefile
--- linux-3.10.14/drivers/dma/Makefile	2013-10-02 00:18:05.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/Makefile	2015-12-31 11:55:16.000000000 +0800
@@ -38,3 +38,6 @@
 obj-$(CONFIG_MMP_TDMA) += mmp_tdma.o
 obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
+obj-$(CONFIG_RT_DMA) += rt_dma.o
+
+
diff -Nru linux-3.10.14/drivers/dma/rt_dma.c linux-3.10.14.new/drivers/dma/rt_dma.c
--- linux-3.10.14/drivers/dma/rt_dma.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/rt_dma.c	2015-12-31 12:16:25.000000000 +0800
@@ -0,0 +1,361 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/pci.h>
+#include <linux/dma-mapping.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/memory.h>
+#include <asm/rt2880/surfboardint.h>
+#include <linux/version.h>
+
+#include "rt_hsdma.h"
+#include "rt_dma.h"
+
+
+/************************ DMA engine API functions ****************************/
+
+#define MEMCPY_DMA_CH	8
+#define to_rt_dma_chan(chan)            \
+	container_of(chan, struct rt_dma_chan, common)
+#ifdef CONFIG_RT_DMA_HSDMA	
+static int hsdma_rx_dma_owner_idx0;
+static int hsdma_rx_calc_idx0;
+static unsigned long hsdma_tx_cpu_owner_idx0=0;
+static unsigned long updateCRX =0;
+#endif
+static dma_cookie_t rt_dma_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	dma_cookie_t cookie;
+	
+	//printk("%s\n",__FUNCTION__);
+  if(tx->chan)
+	cookie = tx->chan->cookie;
+
+	return cookie;
+}
+
+struct HSdmaReqEntry HSDMA_Entry;
+
+#define MIN_RTDMA_PKT_LEN	128
+static struct dma_async_tx_descriptor *
+rt_dma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,
+		size_t len, unsigned long flags)
+{
+	struct rt_dma_chan *rt_chan = to_rt_dma_chan(chan);
+	unsigned long mid_offset;
+#ifdef CONFIG_RT_DMA_HSDMA
+	unsigned long i;
+#endif
+ 
+	//printk("%x->%x len=%d ch=%d\n", src, dest, len, chan->chan_id);
+	spin_lock_bh(&rt_chan->lock);
+
+#ifdef CONFIG_RT_DMA_HSDMA
+  if ((dest & 0x03)!=0){
+  	memcpy(phys_to_virt(dest), phys_to_virt(src), len);	
+	dma_async_tx_descriptor_init(&rt_chan->txd, chan);
+  }
+  else{
+	hsdma_rx_dma_owner_idx0 = (hsdma_rx_calc_idx0 + 1) % NUM_HSDMA_RX_DESC;
+	HSDMA_Entry.HSDMA_tx_ring0[hsdma_tx_cpu_owner_idx0].hsdma_txd_info1.SDP0 = (src & 0xFFFFFFFF);
+	HSDMA_Entry.HSDMA_rx_ring0[hsdma_rx_dma_owner_idx0].hsdma_rxd_info1.PDP0 = (dest & 0xFFFFFFFF);
+  
+	HSDMA_Entry.HSDMA_tx_ring0[hsdma_tx_cpu_owner_idx0].hsdma_txd_info2.SDL0 = len;
+	HSDMA_Entry.HSDMA_rx_ring0[hsdma_rx_dma_owner_idx0].hsdma_rxd_info2.PLEN0 = len;
+	  
+	HSDMA_Entry.HSDMA_tx_ring0[hsdma_tx_cpu_owner_idx0].hsdma_txd_info2.LS0_bit = 1;
+	HSDMA_Entry.HSDMA_tx_ring0[hsdma_tx_cpu_owner_idx0].hsdma_txd_info2.DDONE_bit = 0;
+		
+	hsdma_tx_cpu_owner_idx0 = (hsdma_tx_cpu_owner_idx0+1) % NUM_HSDMA_TX_DESC;
+	hsdma_rx_calc_idx0 = (hsdma_rx_calc_idx0 + 1) % NUM_HSDMA_RX_DESC;
+	sysRegWrite(HSDMA_TX_CTX_IDX0, cpu_to_le32((u32)hsdma_tx_cpu_owner_idx0));
+			
+	dma_async_tx_descriptor_init(&rt_chan->txd, chan);
+	}
+#else
+		mid_offset = len/2;
+		RT_DMA_WRITE_REG(RT_DMA_SRC_REG(MEMCPY_DMA_CH), src);
+		RT_DMA_WRITE_REG(RT_DMA_DST_REG(MEMCPY_DMA_CH), dest);
+		RT_DMA_WRITE_REG(RT_DMA_CTRL_REG(MEMCPY_DMA_CH), (mid_offset << 16) | (3 << 3) | (3 << 0));
+
+		memcpy(phys_to_virt(dest)+mid_offset, phys_to_virt(src)+mid_offset, len-mid_offset);	
+		
+		dma_async_tx_descriptor_init(&rt_chan->txd, chan);
+		
+		while((RT_DMA_READ_REG(RT_DMA_DONEINT) & (0x1<<MEMCPY_DMA_CH))==0);
+		RT_DMA_WRITE_REG(RT_DMA_DONEINT, (1<<MEMCPY_DMA_CH));
+#endif
+
+	spin_unlock_bh(&rt_chan->lock);
+
+	return &rt_chan->txd;
+}
+#ifdef CONFIG_RT_DMA_HSDMA	
+void set_fe_HSDMA_glo_cfg(void)
+{
+	int HSDMA_glo_cfg=0;
+	printk("%s\n",__FUNCTION__);
+	HSDMA_glo_cfg = (HSDMA_TX_WB_DDONE | HSDMA_RX_DMA_EN | HSDMA_TX_DMA_EN | HSDMA_BT_SIZE_16DWORDS | HSDMA_MUTI_ISSUE );
+	sysRegWrite(HSDMA_GLO_CFG, HSDMA_glo_cfg);
+}
+
+static int HSDMA_init(void)
+{
+	int		i;
+	unsigned int	regVal;
+	printk("%s\n",__FUNCTION__);
+	while(1)
+	{
+		regVal = sysRegRead(HSDMA_GLO_CFG);
+		if((regVal & HSDMA_RX_DMA_BUSY))
+		{
+			printk("\n  RX_DMA_BUSY !!! ");
+			continue;
+		}
+		if((regVal & HSDMA_TX_DMA_BUSY))
+		{
+			printk("\n  TX_DMA_BUSY !!! ");
+			continue;
+		}
+		break;
+	}
+	//initial TX ring0
+	HSDMA_Entry.HSDMA_tx_ring0 = pci_alloc_consistent(NULL, NUM_HSDMA_TX_DESC * sizeof(struct HSDMA_txdesc), &HSDMA_Entry.phy_hsdma_tx_ring0);
+	printk("\n hsdma_phy_tx_ring0 = 0x%08x, hsdma_tx_ring0 = 0x%p\n", HSDMA_Entry.phy_hsdma_tx_ring0, HSDMA_Entry.HSDMA_tx_ring0);
+	
+		
+	for (i=0; i < NUM_HSDMA_TX_DESC; i++) {
+		memset(&HSDMA_Entry.HSDMA_tx_ring0[i],0,sizeof(struct HSDMA_txdesc));
+		HSDMA_Entry.HSDMA_tx_ring0[i].hsdma_txd_info2.LS0_bit = 1;
+		HSDMA_Entry.HSDMA_tx_ring0[i].hsdma_txd_info2.DDONE_bit = 1;
+	}
+
+	//initial RX ring0
+	HSDMA_Entry.HSDMA_rx_ring0 = pci_alloc_consistent(NULL, NUM_HSDMA_RX_DESC * sizeof(struct HSDMA_rxdesc), &HSDMA_Entry.phy_hsdma_rx_ring0);
+	
+	
+	for (i = 0; i < NUM_HSDMA_RX_DESC; i++) {
+		memset(&HSDMA_Entry.HSDMA_rx_ring0[i],0,sizeof(struct HSDMA_rxdesc));
+		HSDMA_Entry.HSDMA_rx_ring0[i].hsdma_rxd_info2.DDONE_bit = 0;
+		HSDMA_Entry.HSDMA_rx_ring0[i].hsdma_rxd_info2.LS0 = 0;
+	}	
+		printk("\n hsdma_phy_rx_ring0 = 0x%08x, hsdma_rx_ring0 = 0x%p\n",HSDMA_Entry.phy_hsdma_rx_ring0,HSDMA_Entry.HSDMA_rx_ring0);
+	
+		// HSDMA_GLO_CFG
+		regVal = sysRegRead(HSDMA_GLO_CFG);
+		regVal &= 0x000000FF;
+		sysRegWrite(HSDMA_GLO_CFG, regVal);
+		regVal=sysRegRead(HSDMA_GLO_CFG);
+		/* Tell the adapter where the TX/RX rings are located. */
+		//TX0
+    sysRegWrite(HSDMA_TX_BASE_PTR0, phys_to_bus((u32) HSDMA_Entry.phy_hsdma_tx_ring0));
+		sysRegWrite(HSDMA_TX_MAX_CNT0, cpu_to_le32((u32) NUM_HSDMA_TX_DESC));
+		sysRegWrite(HSDMA_TX_CTX_IDX0, 0);
+		hsdma_tx_cpu_owner_idx0 = 0;
+		sysRegWrite(HSDMA_RST_CFG, HSDMA_PST_DTX_IDX0);
+		printk("TX_CTX_IDX0 = %x\n", sysRegRead(HSDMA_TX_CTX_IDX0));
+	  printk("TX_DTX_IDX0 = %x\n", sysRegRead(HSDMA_TX_DTX_IDX0));
+
+	    
+		//RX0
+		sysRegWrite(HSDMA_RX_BASE_PTR0, phys_to_bus((u32) HSDMA_Entry.phy_hsdma_rx_ring0));
+		sysRegWrite(HSDMA_RX_MAX_CNT0,  cpu_to_le32((u32) NUM_HSDMA_RX_DESC));
+		sysRegWrite(HSDMA_RX_CALC_IDX0, cpu_to_le32((u32) (NUM_HSDMA_RX_DESC - 1)));
+		hsdma_rx_calc_idx0 = hsdma_rx_dma_owner_idx0 =  sysRegRead(HSDMA_RX_CALC_IDX0);
+		sysRegWrite(HSDMA_RST_CFG, HSDMA_PST_DRX_IDX0);
+		printk("RX_CRX_IDX0 = %x\n", sysRegRead(HSDMA_RX_CALC_IDX0));
+		printk("RX_DRX_IDX0 = %x\n", sysRegRead(HSDMA_RX_DRX_IDX0));
+
+		set_fe_HSDMA_glo_cfg();
+		printk("HSDMA_GLO_CFG = %x\n", sysRegRead(HSDMA_GLO_CFG));
+		return 1;
+}
+#endif
+
+int hsdma_housekeeping(void)
+{
+	int i;
+
+	i = (sysRegRead(HSDMA_RX_CALC_IDX0)+1)%NUM_HSDMA_RX_DESC;	   
+
+	while (1) {
+		if (HSDMA_Entry.HSDMA_rx_ring0[i].hsdma_rxd_info2.DDONE_bit == 1) { 
+			HSDMA_Entry.HSDMA_rx_ring0[i].hsdma_rxd_info2.DDONE_bit = 0; // RX_Done_bit=1->0
+			updateCRX=i;
+			i = (i + 1)%NUM_HSDMA_RX_DESC;
+		}	
+		else {
+			break;
+		}
+	} 
+	sysRegWrite(HSDMA_RX_CALC_IDX0, cpu_to_le32((u32)updateCRX)); //update RX CPU IDX 
+
+	return 0;
+}
+
+/**
+ * rt_dma_status - poll the status of an XOR transaction
+ * @chan: XOR channel handle
+ * @cookie: XOR transaction identifier
+ * @txstate: XOR transactions state holder (or NULL)
+ */
+static enum dma_status rt_dma_status(struct dma_chan *chan,
+					  dma_cookie_t cookie,
+					  struct dma_tx_state *txstate)
+{
+	//printk("%s\n",__FUNCTION__);
+	hsdma_housekeeping();
+
+	return 0;
+}
+
+
+static irqreturn_t rt_dma_interrupt_handler(int irq, void *data)
+{
+	//printk("%s\n",__FUNCTION__);
+
+	return IRQ_HANDLED;
+}
+
+static void rt_dma_issue_pending(struct dma_chan *chan)
+{
+	//printk("%s\n",__FUNCTION__);
+}
+
+
+static int rt_dma_alloc_chan_resources(struct dma_chan *chan)
+{
+	//("%s\n",__FUNCTION__);
+
+	return 0;
+}
+
+static void rt_dma_free_chan_resources(struct dma_chan *chan)
+{
+	//printk("%s\n",__FUNCTION__);
+
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,10,0)
+static int rt_dma_probe(struct platform_device *pdev)
+#else
+static int __devinit rt_dma_probe(struct platform_device *pdev)
+#endif
+{
+	struct dma_device *dma_dev;
+	struct rt_dma_chan *rt_chan;
+	int err;
+	int ret;
+#ifdef CONFIG_RT_DMA_HSDMA
+	unsigned long reg_int_mask=0;
+#else
+	int reg;
+#endif
+
+	//printk("%s\n",__FUNCTION__);
+	
+	dma_dev = devm_kzalloc(&pdev->dev, sizeof(*dma_dev), GFP_KERNEL);
+	if (!dma_dev)
+		return -ENOMEM;
+
+
+	INIT_LIST_HEAD(&dma_dev->channels);
+	dma_cap_zero(dma_dev->cap_mask);
+	dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
+	//dma_cap_set(DMA_SLAVE, dma_dev->cap_mask);
+	dma_dev->device_alloc_chan_resources = rt_dma_alloc_chan_resources;
+	dma_dev->device_free_chan_resources = rt_dma_free_chan_resources;
+	dma_dev->device_tx_status = rt_dma_status;
+	dma_dev->device_issue_pending = rt_dma_issue_pending;
+	dma_dev->device_prep_dma_memcpy = rt_dma_prep_dma_memcpy;
+	dma_dev->dev = &pdev->dev;
+
+	rt_chan = devm_kzalloc(&pdev->dev, sizeof(*rt_chan), GFP_KERNEL);
+        if (!rt_chan) {
+		return -ENOMEM;
+	}
+
+	spin_lock_init(&rt_chan->lock);	
+  INIT_LIST_HEAD(&rt_chan->chain);
+	INIT_LIST_HEAD(&rt_chan->completed_slots);
+	INIT_LIST_HEAD(&rt_chan->all_slots);
+	rt_chan->common.device = dma_dev;
+	rt_chan->txd.tx_submit = rt_dma_tx_submit;
+
+	list_add_tail(&rt_chan->common.device_node, &dma_dev->channels);
+
+	err = dma_async_device_register(dma_dev);
+
+	if (0 != err) {
+		pr_err("ERR_MDMA:device_register failed: %d\n", err);
+		return 1;
+	}
+	
+#ifdef CONFIG_RT_DMA_HSDMA
+	ret = request_irq(SURFBOARDINT_HSGDMA, rt_dma_interrupt_handler, IRQF_DISABLED, "HS_DMA", NULL);
+#else
+	ret = request_irq(SURFBOARDINT_DMA, rt_dma_interrupt_handler, IRQF_DISABLED, "GDMA", NULL);
+#endif
+	if(ret){
+		pr_err("IRQ %d is not free.\n", SURFBOARDINT_DMA);
+		return 1;
+	}
+
+#ifdef CONFIG_RT_DMA_HSDMA
+		sysRegWrite(HSDMA_INT_MASK, reg_int_mask  & ~(HSDMA_FE_INT_TX));  // disable int TX DONE
+		sysRegWrite(HSDMA_INT_MASK, reg_int_mask  & ~(HSDMA_FE_INT_RX) );  // disable int RX DONE		
+		printk("reg_int_mask=%lu, INT_MASK= %x \n", reg_int_mask, sysRegRead(HSDMA_INT_MASK));
+  	HSDMA_init();	
+#else
+	//set GDMA register in advance.
+	reg = (32 << 16) | (32 << 8) | (MEMCPY_DMA_CH << 3);
+	RT_DMA_WRITE_REG(RT_DMA_CTRL_REG1(MEMCPY_DMA_CH), reg);
+#endif
+
+	return 0;
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,10,0)
+static int  rt_dma_remove(struct platform_device *dev)
+#else
+static int __devexit rt_dma_remove(struct platform_device *dev)
+#endif
+{
+	struct dma_device *dma_dev = platform_get_drvdata(dev);
+
+	//printk("%s\n",__FUNCTION__);
+
+	dma_async_device_unregister(dma_dev);
+
+	return 0;
+}
+
+static struct platform_driver rt_dma_driver = {
+	.probe		= rt_dma_probe,
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,10,0)
+	.remove		= rt_dma_remove,
+#else
+        .remove         = __devexit_p(rt_dma_remove),
+#endif
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= RT_DMA_NAME,
+	},
+};
+
+static int __init rt_dma_init(void)
+{
+	int rc;
+ // printk("%s\n",__FUNCTION__);
+	rc = platform_driver_register(&rt_dma_driver);
+	return rc;
+}
+module_init(rt_dma_init);
+
+
+MODULE_AUTHOR("Steven Liu <steven_liu@mediatek.com>");
+MODULE_DESCRIPTION("DMA engine driver for Ralink DMA engine");
+MODULE_LICENSE("GPL");
diff -Nru linux-3.10.14/drivers/dma/rt_dma.h linux-3.10.14.new/drivers/dma/rt_dma.h
--- linux-3.10.14/drivers/dma/rt_dma.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/rt_dma.h	2015-12-31 12:16:25.000000000 +0800
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2007, 2008, Marvell International Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#ifndef RT_DMA_H
+#define RT_DMA_H
+
+#include <linux/types.h>
+#include <linux/io.h>
+#include <linux/dmaengine.h>
+#include <linux/interrupt.h>
+#include <asm/rt2880/rt_mmap.h>
+
+#define RT_DMA_NAME             "rt_dma"
+
+struct rt_dma_chan {
+        int                     pending;
+        dma_cookie_t            completed_cookie;
+        spinlock_t              lock; /* protects the descriptor slot pool */
+        void __iomem            *mmr_base;
+        unsigned int            idx;
+        enum dma_transaction_type       current_type;
+	struct dma_async_tx_descriptor txd;
+        struct list_head        chain;
+        struct list_head        completed_slots;
+        struct dma_chan         common;
+        struct list_head        all_slots;
+        int                     slots_allocated;
+        struct tasklet_struct   irq_tasklet;
+};
+
+#define RT_DMA_READ_REG(addr)             le32_to_cpu(*(volatile u32 *)(addr))
+#define RT_DMA_WRITE_REG(addr, val)       *((volatile uint32_t *)(addr)) = cpu_to_le32(val)
+
+#define RT_DMA_SRC_REG(ch)                (RALINK_GDMA_BASE + ch*16)
+#define RT_DMA_DST_REG(ch)                (RT_DMA_SRC_REG(ch) + 4)
+#define RT_DMA_CTRL_REG(ch)               (RT_DMA_DST_REG(ch) + 4)
+#define RT_DMA_CTRL_REG1(ch)              (RT_DMA_CTRL_REG(ch) + 4)
+#define RT_DMA_DONEINT			  (RALINK_GDMA_BASE + 0x204)
+
+#endif
diff -Nru linux-3.10.14/drivers/dma/rt_hsdma.h linux-3.10.14.new/drivers/dma/rt_hsdma.h
--- linux-3.10.14/drivers/dma/rt_hsdma.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-3.10.14.new/drivers/dma/rt_hsdma.h	2015-12-31 12:16:27.000000000 +0800
@@ -0,0 +1,204 @@
+#ifndef _RALINK_HSDMA
+#define _RALINK_HSDMA
+
+#include <asm/rt2880/rt_mmap.h>
+
+
+#define NUM_HSDMA_RX_DESC     1024
+#define NUM_HSDMA_TX_DESC     1024
+
+#define phys_to_bus(a) (a & 0x1FFFFFFF)
+
+#define PHYS_TO_K1(physaddr) KSEG1ADDR(physaddr)
+
+
+#define sysRegRead(phys)        \
+        (*(volatile unsigned int *)PHYS_TO_K1(phys))
+
+#define sysRegWrite(phys, val)  \
+        ((*(volatile unsigned int *)PHYS_TO_K1(phys)) = (val))
+
+
+
+#define u_long	unsigned long
+#define u32	unsigned int
+#define u16	unsigned short
+
+
+#define HSDMA_RELATED            0x0800
+/* 1. HSDMA */
+#define HSDMA_TX_BASE_PTR0            (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x000)
+#define HSDMA_TX_MAX_CNT0             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x004)
+#define HSDMA_TX_CTX_IDX0             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x008)
+#define HSDMA_TX_DTX_IDX0             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x00C)
+
+#define HSDMA_RX_BASE_PTR0            (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x100)
+#define HSDMA_RX_MAX_CNT0             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x104)
+#define HSDMA_RX_CALC_IDX0            (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x108)
+#define HSDMA_RX_DRX_IDX0             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x10C)
+
+#define HSDMA_INFO                    (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x200)
+#define HSDMA_GLO_CFG 								(RALINK_HS_DMA_BASE + HSDMA_RELATED+0x204)
+#define HSDMA_RST_IDX            			(RALINK_HS_DMA_BASE + HSDMA_RELATED+0x208)
+#define HSDMA_RST_CFG            			(HSDMA_RST_IDX)
+#define HSDMA_DLY_INT_CFG             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x20C)
+#define HSDMA_FREEQ_THRES             (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x210)
+#define HSDMA_INT_STATUS              (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x220)
+#define HSDMA_FE_INT_STATUS		        (HSDMA_INT_STATUS)
+#define HSDMA_INT_MASK                (RALINK_HS_DMA_BASE + HSDMA_RELATED+0x228)
+//#define FE_INT_ENABLE		(INT_MASK)
+#define SCH_Q01_CFG		(RALINK_HS_DMA_BASE+RAHSDMA_OFFSET+0x280)
+#define SCH_Q23_CFG		(RALINK_HS_DMA_BASE+RAHSDMA_OFFSET+0x284)
+
+/* ====================================== */
+/* ====================================== */
+#define GP1_LNK_DWN     (1<<9)
+#define GP1_AN_FAIL     (1<<8) 
+/* ====================================== */
+/* ====================================== */
+#define PSE_RESET       (1<<0)
+/* ====================================== */
+#define HSDMA_PST_DRX_IDX1       (1<<17)
+#define HSDMA_PST_DRX_IDX0       (1<<16)
+#define HSDMA_PST_DTX_IDX3       (1<<3)
+#define HSDMA_PST_DTX_IDX2       (1<<2)
+#define HSDMA_PST_DTX_IDX1       (1<<1)
+#define HSDMA_PST_DTX_IDX0       (1<<0)
+
+#define HSDMA_RX_2B_OFFSET	    (1<<31)
+#define HSDMA_MUTI_ISSUE        (1<<10)
+#define HSDMA_TX_WB_DDONE       (1<<6)
+#define HSDMA_RX_DMA_BUSY       (1<<3)
+#define HSDMA_TX_DMA_BUSY       (1<<1)
+#define HSDMA_RX_DMA_EN         (1<<2)
+#define HSDMA_TX_DMA_EN         (1<<0)
+
+#define HSDMA_BT_SIZE_4DWORDS     (0<<4)
+#define HSDMA_BT_SIZE_8DWORDS     (1<<4)
+#define HSDMA_BT_SIZE_16DWORDS    (2<<4)
+#define HSDMA_BT_SIZE_32DWORDS    (3<<4)
+
+#define HSDMA_RX_COHERENT      (1<<31)
+#define HSDMA_RX_DLY_INT       (1<<30)
+#define HSDMA_TX_COHERENT      (1<<29)
+#define HSDMA_TX_DLY_INT       (1<<28)
+#define HSDMA_RX_DONE_INT0     (1<<16)
+#define HSDMA_TX_DONE_INT0     (1)
+#define HSDMA_TX_DONE_BIT      (1<<31)
+#define HSDMA_TX_LS0           (1<<30)
+
+#define HSDMA_FE_INT_ALL    (HSDMA_TX_DONE_INT0 | HSDMA_RX_DONE_INT0)                        
+#define HSDMA_FE_INT_TX			(HSDMA_TX_DONE_INT0)
+#define HSDMA_FE_INT_RX     (HSDMA_RX_DONE_INT0)
+#define HSDMA_FE_INT_STATUS_REG (*(volatile unsigned long *)(FE_INT_STATUS))
+#define HSDMA_FE_INT_STATUS_CLEAN(reg) (*(volatile unsigned long *)(FE_INT_STATUS)) = reg
+
+// Define Whole FE Reset Register
+#define RSTCTRL         (RALINK_SYSCTL_BASE + 0x34)
+/*=========================================
+      HSDMA HSDMA_RX Descriptor Format define
+=========================================*/
+
+//-------------------------------------------------
+typedef struct _HSDMA_RXD_INFO1_  HSDMA_RXD_INFO1_T;
+
+struct _HSDMA_RXD_INFO1_
+{
+    unsigned int    PDP0;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_RXD_INFO2_    HSDMA_RXD_INFO2_T;
+
+struct _HSDMA_RXD_INFO2_
+{
+    volatile unsigned int    PLEN1                 : 14;
+    volatile unsigned int    LS1                   : 1;
+    volatile unsigned int    TAG                   : 1;
+    volatile unsigned int    PLEN0                 : 14;
+    volatile unsigned int    LS0                   : 1;
+    volatile unsigned int    DDONE_bit             : 1;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_RXD_INFO3_  HSDMA_RXD_INFO3_T;
+
+struct _HSDMA_RXD_INFO3_
+{
+    unsigned int    PDP1;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_RXD_INFO4_    HSDMA_RXD_INFO4_T;
+
+struct _HSDMA_RXD_INFO4_
+{
+	 unsigned int	unused : 32;
+};
+
+
+struct HSDMA_rxdesc {
+	HSDMA_RXD_INFO1_T hsdma_rxd_info1;
+	HSDMA_RXD_INFO2_T hsdma_rxd_info2;
+	HSDMA_RXD_INFO3_T hsdma_rxd_info3;
+	HSDMA_RXD_INFO4_T hsdma_rxd_info4;
+};
+
+/*=========================================
+      HSDMA HSDMA_TX Descriptor Format define
+=========================================*/
+//-------------------------------------------------
+typedef struct _HSDMA_TXD_INFO1_  HSDMA_TXD_INFO1_T;
+
+struct _HSDMA_TXD_INFO1_
+{
+    unsigned int    SDP0;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_TXD_INFO2_    HSDMA_TXD_INFO2_T;
+
+struct _HSDMA_TXD_INFO2_
+{
+    volatile unsigned int    SDL1                  : 14;
+    volatile unsigned int    LS1_bit               : 1;
+    volatile unsigned int    BURST_bit             : 1;
+    volatile unsigned int    SDL0                  : 14;
+    volatile unsigned int    LS0_bit               : 1;
+    volatile unsigned int    DDONE_bit             : 1;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_TXD_INFO3_  HSDMA_TXD_INFO3_T;
+
+struct _HSDMA_TXD_INFO3_
+{
+    unsigned int    SDP1;
+};
+//-------------------------------------------------
+typedef struct _HSDMA_TXD_INFO4_    HSDMA_TXD_INFO4_T;
+
+struct _HSDMA_TXD_INFO4_
+{
+		unsigned int	unused : 32;
+};
+
+
+struct HSDMA_txdesc {
+	HSDMA_TXD_INFO1_T hsdma_txd_info1;
+	HSDMA_TXD_INFO2_T hsdma_txd_info2;
+	HSDMA_TXD_INFO3_T hsdma_txd_info3;
+	HSDMA_TXD_INFO4_T hsdma_txd_info4;
+};
+
+
+struct HSdmaReqEntry {
+
+    unsigned int  hsdma_tx_full;
+    unsigned int	phy_hsdma_tx_ring0;
+    unsigned int	phy_hsdma_rx_ring0;
+    spinlock_t          page_lock;              /* Page register locks */
+    struct HSDMA_txdesc *HSDMA_tx_ring0;
+
+    struct HSDMA_rxdesc *HSDMA_rx_ring0;
+		void (*DoneIntCallback)(uint32_t);
+		struct work_struct  reset_task;
+};
+
+
+#endif
